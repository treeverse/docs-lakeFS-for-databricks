{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "CLI Reference",
    "title": "CLI Reference",
    "content": " ",
    "url": "/CLI%20Reference.html",
    
    "relUrl": "/CLI%20Reference.html"
  },"3": {
    "doc": "AWS",
    "title": "lakeFS for Databricks on AWS",
    "content": "In this guide we will do a step-by-step walkthrough of setting up an environment of lakeFS cloud and a unity catalog on AWS. At the end of this guide you will be able to use unity catalog while using lakeFS as the backend storage. ",
    "url": "/getstarted/setup/aws.html#lakefs-for-databricks-on-aws",
    
    "relUrl": "/getstarted/setup/aws.html#lakefs-for-databricks-on-aws"
  },"4": {
    "doc": "AWS",
    "title": "Pre-requisites",
    "content": ". | An AWS account, with the necessary permissions to create resources such as IAM Role, IAM User and S3 Bucket | A Databricks workspace with unity enabled and permissions to create a token for managing a catalog | . ",
    "url": "/getstarted/setup/aws.html#pre-requisites",
    
    "relUrl": "/getstarted/setup/aws.html#pre-requisites"
  },"5": {
    "doc": "AWS",
    "title": "Step 1: Setting up lakeFS Cloud on AWS",
    "content": "To learn more about how to set lakeFS cloud see our blog post. | Log in to lakefs.cloud. If you don’t have an account: . | Complete the registration process. | Verify your email. | . | Go to Onboarding -&gt; Setup and follow the setup steps. Keep the following points in mind to ensure your setup integrates with your Databricks workspace: . | In Step 1: | . | Cloud Vendor: Choose “AWS” | Region: Choose the same region as your Databricks workspace. - In Step 3: | The S3 bucket Should be a path in the bucket that will be used for the lakeFS repos (e.g. s3://my-bucket/lakefs) while at the same time will be configured as an external location in Databricks. | Note: Garbage Collection is currently not supported by lakeFS for Databricks, so do not enable it. - In Step 2/4: Set an IAM Role or Credentials for lakeFS: The permissions should cover the full external storage location in databricks that you will create in later step (for all repos) See IAM for lakeFS | . | Verify your setup is complete: . | Go to Clusters and click on the link to the newly created installation. | Log in to your lakeFS instance. | . If you’re able to log in, your setup is complete. | . ",
    "url": "/getstarted/setup/aws.html#step-1-setting-up-lakefs-cloud-on-aws",
    
    "relUrl": "/getstarted/setup/aws.html#step-1-setting-up-lakefs-cloud-on-aws"
  },"6": {
    "doc": "AWS",
    "title": "Step 2: Unity Configurations",
    "content": ". | Create an External location as the storage root for the catalogs you will create using lakeFS for Databricks (i.e s3://my-bucket/lakefs). | . Make sure Databricks has Storage Credentials to access the external location you will use. | A personal access token for a Databricks user with the following permissions: . | CREATE MANAGED STORAGE for the specified external location. | CREATE CATALOG for the workspace’s metastore to enable catalog creation. | . | . ",
    "url": "/getstarted/setup/aws.html#step-2-unity-configurations",
    
    "relUrl": "/getstarted/setup/aws.html#step-2-unity-configurations"
  },"7": {
    "doc": "AWS",
    "title": "Step 3: Create dedicated lakeFS repository for Unity Catalog",
    "content": "In lakefs-databricks, there is a 1:1 mapping between a lakeFS repository and a single Unity Catalog. Before creating a catalog, you need to create a dedicated lakeFS repository for the catalog. Creating a repository can be done via the lakeFS UI or via the lakectl. The S3 underlying location will be used later as a prefix for tables created in Unity Catalog. Example with lakectl: . lakectl repo create lakefs://&lt;lakefs-repo-name&gt; s3://my-bucket/lakefs/repos/&lt;lakefs-repo-name&gt; . ",
    "url": "/getstarted/setup/aws.html#step-3-create-dedicated-lakefs-repository-for-unity-catalog",
    
    "relUrl": "/getstarted/setup/aws.html#step-3-create-dedicated-lakefs-repository-for-unity-catalog"
  },"8": {
    "doc": "AWS",
    "title": "Step 4: Setup lakefs-databricks",
    "content": "Now that you have a lakeFS repository, you can setup lakefs-databricks to create a catalog and tables in Databricks. | Download CLI (TODO: Add link) . | Configure lakefs-databricks, the default location is $HOME/.lakefs-databricks.yaml . | lakeFS credentials to access the lakeFS API. | Unity: url and token to access the Databricks workspace. | AWS S3 credentials to access the S3 bucket where the lakeFS repo / Unity catalog is located. | . | . The S3 Credentials will be used to access the underlying storage of the tables created in Unity Catalog. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\": \"arn:aws:s3:::my-bucket/lakefs/repos/*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::bucket\", \"Condition\": { \"StringLike\": { \"s3:prefix\": \"lakefs/repos*\" } } } ] } . To create a catalog with sample data you will need to allow lakefs-databricks access to public buckets example with no access in the caller account. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"StringNotEquals\": { \"aws:PrincipalAccount\": \"YOUR_ACCOUNT_ID\" } } } ] } . Finally this is how the configuration file should look like in lakefs-databricks.yaml: . unity: # The Databricks workspace URL (i.e https://dbc-&lt;workspace-id&gt;.cloud.databricks.com) url: &lt;unity_url&gt; token: &lt;unity_token&gt; lakefs: secret_access_key: &lt;lakefs_secret_access_key&gt; access_key_id: &lt;lakefs_access_key_id&gt; # Cloud Installation API URL suffix /api/v1 url: &lt;lakefs_installation_url&gt;/api/v1 databricks_storage: type: s3 s3: access_key_id: &lt;aws_access_key_id&gt; secret_access_key: &lt;aws_secret_access_key&gt; region: &lt;aws_region&gt; . ",
    "url": "/getstarted/setup/aws.html#step-4-setup-lakefs-databricks",
    
    "relUrl": "/getstarted/setup/aws.html#step-4-setup-lakefs-databricks"
  },"9": {
    "doc": "AWS",
    "title": "Step 5: Create Catalog",
    "content": "# default config location $HOME/.lakefs-databricks.yaml lakefs-databricks catalog create &lt;catalog-name&gt; &lt;lakefs-repository-name&gt; . That’s it! You have successfully created a catalog in Databricks that is backed by lakeFS. You can now continue to create tables in the catalog. ",
    "url": "/getstarted/setup/aws.html#step-5-create-catalog",
    
    "relUrl": "/getstarted/setup/aws.html#step-5-create-catalog"
  },"10": {
    "doc": "AWS",
    "title": "Create Table (Out of scope) - Go to differet section",
    "content": " ",
    "url": "/getstarted/setup/aws.html#create-table-out-of-scope---go-to-differet-section",
    
    "relUrl": "/getstarted/setup/aws.html#create-table-out-of-scope---go-to-differet-section"
  },"11": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/getstarted/setup/aws.html",
    
    "relUrl": "/getstarted/setup/aws.html"
  },"12": {
    "doc": "Azure",
    "title": "lakeFS for Databricks on Azure",
    "content": "In this guide we will do a step-by-step walkthrough of setting up an environment of lakeFS cloud and a Databricks Workspace on Azure. By the end of this guide, you will be able to use Unity Catalog with lakeFS’s versioning capabilities. ",
    "url": "/getstarted/setup/azure.html#lakefs-for-databricks-on-azure",
    
    "relUrl": "/getstarted/setup/azure.html#lakefs-for-databricks-on-azure"
  },"13": {
    "doc": "Azure",
    "title": "Pre-requisites",
    "content": ". | A Databricks workspace with Unity Catalog enabled. | Define an External location as the storage root for the catalogs you will create using lakeFS for Databricks. | The workspace you are using should have access to this location. | . | Databricks Storage Credentials to access the external location you will use. | The workspace you are using should have access to these storage credentials. | . | A personal access token for a Databricks user with the following permissions: . | CREATE MANAGED STORAGE for the specified external location. | CREATE CATALOG for the workspace’s metastore to enable catalog creation. | . | An Azure account, with the necessary permissions to create Azure AD applications and service principals. | . ",
    "url": "/getstarted/setup/azure.html#pre-requisites",
    
    "relUrl": "/getstarted/setup/azure.html#pre-requisites"
  },"14": {
    "doc": "Azure",
    "title": "Setting up lakeFS Cloud on Azure",
    "content": ". | Log in to lakefs.cloud. If you don’t have an account: . | Complete the registration process. | Verify your email. | . | Go to Onboarding -&gt; Setup and follow the setup steps. Keep the following points in mind to ensure your setup integrates with your Databricks workspace: . | In Step 1: | . | Cloud Vendor: Choose “Azure” | Region: Choose the same region as your Databricks workspace. - In Step 3: | Fill in your storage group, storage account, and container as specified in the “Prerequisites” section. | Use the Storage Account and Container designated for the external location in Databricks. | Note: Garbage Collection is currently not supported by lakeFS for Databricks, so do not enable it. | . | Verify your setup is complete: . | Go to Clusters and click on the link to the newly created installation. | Log in to your lakeFS instance. | . If you’re able to log in, your setup is complete. | . ",
    "url": "/getstarted/setup/azure.html#setting-up-lakefs-cloud-on-azure",
    
    "relUrl": "/getstarted/setup/azure.html#setting-up-lakefs-cloud-on-azure"
  },"15": {
    "doc": "Azure",
    "title": "Create a dedicated lakeFS repository for a Unity catalog",
    "content": ". | Create a new repository in lakeFS through the UI or lakectl CLI. The namespace for your new repository should be accessible through the created external location. | . Steps for creating a new repository through the lakeFS UI: . 1Log in to lakeFS - Open your web browser and navigate to your lakeFS instance URL. - Log in with your credentials. | Access the Repositories Page . | Once logged in, you should see a list of repositories (if any exist). | Look for a button or link labeled “Create Repository”. | . | Fill in Repository Details . | Enter a unique name for your repository. | Choose a storage namespace (the namespace for your new repository should be under the external location you created in the pre-requisites). | Click “Create Repository”. | . | . Congratulations! You’ve successfully created a new repository in lakeFS. ",
    "url": "/getstarted/setup/azure.html#create-a-dedicated-lakefs-repository-for-a-unity-catalog",
    
    "relUrl": "/getstarted/setup/azure.html#create-a-dedicated-lakefs-repository-for-a-unity-catalog"
  },"16": {
    "doc": "Azure",
    "title": "Generate lakeFS credentials",
    "content": "Through the lakeFS UI, generate a new access key for your user. This key will be used to authenticate with lakeFS for Databricks. Steps for generating a new access key through the lakeFS UI: . | Access the Administration page . | Click on the “Administration” link in the top navigation bar. | You should see an empty list of credentials ath the “My Credentials” tab. | . | Generate a new access key . | Click on the “Generate Access Key” button. | Enter a description for the new access key. | Click “Generate Access Key”. | . | . Congratulations! You’ve successfully generated a new access key for your user. Copy the access key and secret key to a secure location. In case you want to use a user with limited permissions, you can create an API user with the following permissions: . { \"id\": \"lakeFSForDatabricksPolicy\", \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:ReadTag\", \"fs:CreateBranch\", \"fs:CreateTag\", \"fs:DeleteBranch\", \"fs:DeleteTag\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/{repositoryId}\" } ] } . You can read more about lakeFS RBAC here. ",
    "url": "/getstarted/setup/azure.html#generate-lakefs-credentials",
    
    "relUrl": "/getstarted/setup/azure.html#generate-lakefs-credentials"
  },"17": {
    "doc": "Azure",
    "title": "Setup lakefs-databricks",
    "content": ". | Download CLI (how?) | . ",
    "url": "/getstarted/setup/azure.html#setup-lakefs-databricks",
    
    "relUrl": "/getstarted/setup/azure.html#setup-lakefs-databricks"
  },"18": {
    "doc": "Azure",
    "title": "lakefs-databricks Configuration",
    "content": "unity: # The Databricks workspace URL (i.e https://dbc-&lt;workspace-id&gt;.cloud.databricks.com) url: &lt;unity_url&gt; token: &lt;unity_token&gt; lakefs: secret_access_key: &lt;lakefs_secret_access_key&gt; access_key_id: &lt;lakefs_access_key_id&gt; # Cloud Installation API URL suffix /api/v1 url: &lt;lakefs_installation_url&gt;/api/v1 databricks_storage: type: azure azure: client_id: &lt;client_id&gt; client_secret: &lt;client_secret&gt; tenant_id: &lt;tenant_id&gt; . ",
    "url": "/getstarted/setup/azure.html#lakefs-databricks-configuration",
    
    "relUrl": "/getstarted/setup/azure.html#lakefs-databricks-configuration"
  },"19": {
    "doc": "Azure",
    "title": "Create Catalog",
    "content": "# default config location $HOME/.lakefs-databricks.yaml lakefs-databricks catalog create &lt;catalog-name&gt; &lt;lakefs-repository-name&gt; . ",
    "url": "/getstarted/setup/azure.html#create-catalog",
    
    "relUrl": "/getstarted/setup/azure.html#create-catalog"
  },"20": {
    "doc": "Azure",
    "title": "Azure",
    "content": " ",
    "url": "/getstarted/setup/azure.html",
    
    "relUrl": "/getstarted/setup/azure.html"
  },"21": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/faq.html",
    
    "relUrl": "/faq.html"
  },"22": {
    "doc": "Get Started",
    "title": "Get Started with lakeFS for Databricks",
    "content": " ",
    "url": "/getstarted/#get-started-with-lakefs-for-databricks",
    
    "relUrl": "/getstarted/#get-started-with-lakefs-for-databricks"
  },"23": {
    "doc": "Get Started",
    "title": "Install",
    "content": " ",
    "url": "/getstarted/#install",
    
    "relUrl": "/getstarted/#install"
  },"24": {
    "doc": "Get Started",
    "title": "Setup",
    "content": " ",
    "url": "/getstarted/#setup",
    
    "relUrl": "/getstarted/#setup"
  },"25": {
    "doc": "Get Started",
    "title": "Try it Out",
    "content": " ",
    "url": "/getstarted/#try-it-out",
    
    "relUrl": "/getstarted/#try-it-out"
  },"26": {
    "doc": "Get Started",
    "title": "Get Started",
    "content": " ",
    "url": "/getstarted/",
    
    "relUrl": "/getstarted/"
  },"27": {
    "doc": "lakeFS for Databricks",
    "title": "Welcome to lakeFS for Databricks!",
    "content": ". ",
    "url": "/#welcome-to-lakefs-for-databricks",
    
    "relUrl": "/#welcome-to-lakefs-for-databricks"
  },"28": {
    "doc": "lakeFS for Databricks",
    "title": "What is lakeFS for Databricks?",
    "content": " ",
    "url": "/#what-is-lakefs-for-databricks",
    
    "relUrl": "/#what-is-lakefs-for-databricks"
  },"29": {
    "doc": "lakeFS for Databricks",
    "title": "Why lakeFS for Databricks?",
    "content": "Use cases . ",
    "url": "/#why-lakefs-for-databricks",
    
    "relUrl": "/#why-lakefs-for-databricks"
  },"30": {
    "doc": "lakeFS for Databricks",
    "title": "How does lakeFS for Databricks work?",
    "content": "Limitations . ",
    "url": "/#how-does-lakefs-for-databricks-work",
    
    "relUrl": "/#how-does-lakefs-for-databricks-work"
  },"31": {
    "doc": "lakeFS for Databricks",
    "title": "lakeFS for Databricks",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"32": {
    "doc": "Install",
    "title": "Install",
    "content": " ",
    "url": "/getstarted/install.html",
    
    "relUrl": "/getstarted/install.html"
  },"33": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/roadmap.html",
    
    "relUrl": "/roadmap.html"
  },"34": {
    "doc": "Setup",
    "title": "Setup",
    "content": " ",
    "url": "/getstarted/setup.html",
    
    "relUrl": "/getstarted/setup.html"
  },"35": {
    "doc": "Try it Out",
    "title": "Try it Out",
    "content": " ",
    "url": "/getstarted/try-it-out.html",
    
    "relUrl": "/getstarted/try-it-out.html"
  },"36": {
    "doc": "Try it Out",
    "title": "With Sample Data",
    "content": " ",
    "url": "/getstarted/try-it-out.html#with-sample-data",
    
    "relUrl": "/getstarted/try-it-out.html#with-sample-data"
  },"37": {
    "doc": "Try it Out",
    "title": "With your Data",
    "content": " ",
    "url": "/getstarted/try-it-out.html#with-your-data",
    
    "relUrl": "/getstarted/try-it-out.html#with-your-data"
  }
}
