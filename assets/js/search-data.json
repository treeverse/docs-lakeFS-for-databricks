{"0": {
    "doc": "Page not found",
    "title": "Page not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"1": {
    "doc": "Page not found",
    "title": "We can’t seem to find the page you’re looking for.",
    "content": "Error code: 404 . Back to Docs . ",
    "url": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for",
    
    "relUrl": "/404.html#we-cant-seem-to-find-the-page-youre-looking-for"
  },"2": {
    "doc": "AWS",
    "title": "Setup lakeFS for Databricks on AWS",
    "content": "Setting up lakeFS for Databricks on AWS involves three steps: . | Prepare Your Databricks Workspace: to enable access to Unity Catalog and External storage locations | Set Up lakeFS Cloud: Configure lakeFS Cloud to version your Unity Catalog data | Configure the lakefs-databricks CLI: configure the lakefs-databricks CLI tool to interact with your Databricks workspace and lakeFS Cloud | . ",
    "url": "/getstarted/setup/aws.html#setup-lakefs-for-databricks-on-aws",
    
    "relUrl": "/getstarted/setup/aws.html#setup-lakefs-for-databricks-on-aws"
  },"3": {
    "doc": "AWS",
    "title": "Table of contents",
    "content": ". | Prerequisites | Prepare your Databricks Workspace | Setup lakeFS Cloud | Configure the lakefs-databricks CLI | . ",
    "url": "/getstarted/setup/aws.html#table-of-contents",
    
    "relUrl": "/getstarted/setup/aws.html#table-of-contents"
  },"4": {
    "doc": "AWS",
    "title": "Prerequisites",
    "content": ". | Databricks workspace with Unity Catalog enabled. | AWS account, with the permissions to create policies and IAM Roles or Users. | Ensure you have an S3 bucket or a specific prefix within a bucket ready for lakeFS to use with Databricks, such as, s3://lakefs-for-databricks or s3://my-bucket/lakefs-for-databricks. lakeFS for Databricks will create catalogs under this container. | . ",
    "url": "/getstarted/setup/aws.html#prerequisites",
    
    "relUrl": "/getstarted/setup/aws.html#prerequisites"
  },"5": {
    "doc": "AWS",
    "title": "Prepare your Databricks Workspace",
    "content": "Create the following resources within your Databricks workspace: . | Storage Credentials: Create Databricks Storage Credentials that provide read and write access to the S3 bucket or prefix selected for lakeFS for Databricks (refer to prerequisites). Ensure your workspace has permission to use these storage credentials. | External Location: Use the storage credentials created in the previous step to define an External Location for a URL within the selected S3 bucket prefix for lakeFS for Databricks (refer to prerequisites), e.g. s3://lakefs-for-databricks/my-first-catalog. Ensure your Databricks workspace has access to this location. Note It is recommended to create a dedicated external location for each catalog lakeFS for Databricks create and manages. | Personal Access Token: Generate a personal access token for a Databricks user with the following permissions: . | CREATE MANAGED STORAGE for the specified external location | CREATE CATALOG for the workspace’s metastore to enable catalog creation | . | . ",
    "url": "/getstarted/setup/aws.html#prepare-your-databricks-workspace",
    
    "relUrl": "/getstarted/setup/aws.html#prepare-your-databricks-workspace"
  },"6": {
    "doc": "AWS",
    "title": "Setup lakeFS Cloud",
    "content": "Step 1: Log in to Your lakeFS Cloud Account . Log in to the lakeFS Cloud account created during the installation process. Step 2: Configure lakeFS Cloud . Follow these steps to configure lakeFS Cloud with the S3 bucket or prefix selected for lakeFS for Databricks (refer to prerequisites):. Navigate to Onboarding -&gt; Setup, and follow the guide to complete the setup wizard to ensure compatibility with lakeFS for Databricks: . Select Cloud Vendor and Region to run the lakeFS Cloud Servers . | Cloud Vendor: Select \"AWS\" | Region: Choose the region where your Databricks Workspace is located | . Select Authentication Method . | Select authentication method: Static credentials or IAM Role | If you selected static credentials, use this guide as a reference for the S3 permissions this user needs to have. Make sure that the user has permissions for the Bucket or Prefix you selected for lakeFS for Databricks as specified in the Prerequisites section. | . Note It is recommended to use IAM Role as it is a more secure option. Create Roles . | Fill in Bucket and Prefix you selected for lakeFS for Databricks as specified in the Prerequisites section. | . Note Garbage Collection is currently unsupported by lakeFS for Databricks. Please disable it during setup. Apply Terraform or CloudFormation templates to create AWS Resources . | Switch to your AWS account and apply the created Terraform or CloudFormation templates to create resources required by lakeFS Cloud | . Connectivity Test . | Test your connection! | . Step 3: Log in to your lakeFS Cloud Installation . Log in to the installation created during the setup process you’ve followed: . | Navigate to Clusters and click on the link to the newly created installation. | Log in to your lakeFS instance. | . Step 4: Generate lakeFS Credentials . To generate credentials for lakeFS for Databricks: . | In the lakeFS UI, go to the Administration tab. | Click “Create Access Key” | Save the access key and secret key in a secure location | By default, these credentials have admin permissions. If you need more restrictive permissions, create a user with the following steps: . | Go to Administration -&gt; Users | Click “Create API User” and name it “lakeFS for Databricks” | Go to Administration -&gt; Policies and create the policy below: { \"id\": \"lakeFSForDatabricksPolicy\", \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:ReadTag\", \"fs:CreateBranch\", \"fs:CreateTag\", \"fs:DeleteBranch\", \"fs:DeleteTag\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/{repositoryId}\" } ] } . | Go to Administration -&gt; Users | Select “lakeFS for Databricks” | Attach the newly created lakeFSForDatabricksPolicy | . | . ",
    "url": "/getstarted/setup/aws.html#setup-lakefs-cloud",
    
    "relUrl": "/getstarted/setup/aws.html#setup-lakefs-cloud"
  },"7": {
    "doc": "AWS",
    "title": "Configure the lakefs-databricks CLI",
    "content": "Step 1: Create AWS Static Credentials . lakeFS for Databricks uses static AWS credentials to access the Bucket or prefix that was selected for it (see Prerequisites). The following policies should apply to the user you created the access-key pair for: . Assuming that you selected s3://my-bucket/lakefs-for-databricks for lakeFS for Databricks. the policy below defines the access lakeFS for Databricks needs to the storage: . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:DeleteObject\", \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\": \"arn:aws:s3:::my-bucket/lakefs-for-databricks/*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::my-bucket\", \"Condition\": { \"StringLike\": { \"s3:prefix\": \"lakefs-for-databricks*\" } } } ] } . lakeFS for Databricks enables you to create catalogs with sample data. If you want to use this functionality define the policy below to allow access to public buckets example with no access in the caller account. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"StringNotEquals\": { \"aws:PrincipalAccount\": \"YOUR_ACCOUNT_ID\" } } } ] } . Step 2: Configure the CLI . Follow the lakefs-databricks configuration guide. ",
    "url": "/getstarted/setup/aws.html#configure-the-lakefs-databricks-cli",
    
    "relUrl": "/getstarted/setup/aws.html#configure-the-lakefs-databricks-cli"
  },"8": {
    "doc": "AWS",
    "title": "AWS",
    "content": " ",
    "url": "/getstarted/setup/aws.html",
    
    "relUrl": "/getstarted/setup/aws.html"
  },"9": {
    "doc": "Azure",
    "title": "Setup lakeFS for Databricks on Azure",
    "content": "Setting up lakeFS for Databricks on Azure involves three steps: . | Prepare Your Databricks Workspace: to enable access to Unity Catalog and External storage locations | Set Up lakeFS Cloud: Configure lakeFS Cloud to version your Unity Catalog data | Configure the lakefs-databricks CLI: configure the lakefs-databricks CLI tool to interact with your Databricks workspace and lakeFS Cloud | . ",
    "url": "/getstarted/setup/azure.html#setup-lakefs-for-databricks-on-azure",
    
    "relUrl": "/getstarted/setup/azure.html#setup-lakefs-for-databricks-on-azure"
  },"10": {
    "doc": "Azure",
    "title": "Table of contents",
    "content": ". | Prerequisites | Prepare your Databricks Workspace | Setup lakeFS Cloud | Configure the lakefs-databricks CLI | . ",
    "url": "/getstarted/setup/azure.html#table-of-contents",
    
    "relUrl": "/getstarted/setup/azure.html#table-of-contents"
  },"11": {
    "doc": "Azure",
    "title": "Prerequisites",
    "content": ". | Databricks workspace with Unity Catalog enabled. | Azure account, with the permissions to create Azure AD applications and service principals. | Ensure you have a storage container ready for lakeFS to use with Databricks, such as, https://my-storage-account.blob.core.windows.net/lakefs-databricks-container. lakeFS for Databricks will create catalogs under this container. | . ",
    "url": "/getstarted/setup/azure.html#prerequisites",
    
    "relUrl": "/getstarted/setup/azure.html#prerequisites"
  },"12": {
    "doc": "Azure",
    "title": "Prepare your Databricks Workspace",
    "content": "Create the following resources within your Databricks workspace: . | Storage Credentials: Create Databricks Storage Credentials with associated Storage Blob Data Contributor role for the storage container selected for lakeFS for Databricks (refer to prerequisites). Ensure your workspace has permission to use these storage credentials. | External Location: Use the storage credentials created in the previous step to define an External Location within the selected storage container for lakeFS for Databricks (refer to prerequisites), e.g. https://my-storage-account.blob.core.windows.net/lakefs-databricks-container/my-first-catalog. Ensure your Databricks workspace has access to this location. Note It is recommended to create a dedicated external location for each catalog lakeFS for Databricks create and manages. | Personal Access Token: Generate a personal access token for a Databricks user with the following permissions: . | CREATE MANAGED STORAGE for the specified external location | CREATE CATALOG for the workspace’s metastore to enable catalog creation | . | . ",
    "url": "/getstarted/setup/azure.html#prepare-your-databricks-workspace",
    
    "relUrl": "/getstarted/setup/azure.html#prepare-your-databricks-workspace"
  },"13": {
    "doc": "Azure",
    "title": "Setup lakeFS Cloud",
    "content": "Step 1: Log in to Your lakeFS Cloud Account . Log in to the lakeFS Cloud account created during the installation process. Step 2: Configure lakeFS Cloud . Follow these steps to configure lakeFS Cloud with the storage container selected for lakeFS for Databricks (refer to prerequisites):. Navigate to Onboarding -&gt; Setup, and follow the guide to complete the setup wizard to ensure compatibility with lakeFS for Databricks: . Select Cloud Vendor and Region to run the lakeFS Cloud Servers . | Cloud Vendor: Select \"Azure\" | Region: Choose the region where your Databricks Workspace is located | . Create Roles . | Fill in the Storage Group, Storage Account, and Container you selected for lakeFS for Databricks as specified in the Prerequisites section. | . Note Garbage Collection is currently unsupported by lakeFS for Databricks. Please disable it during setup. Apply Terraform to create Azure Resources . | Switch to your Azure account and apply the created Terraform template to create resources required by lakeFS Cloud | The Terraform output includes Service Principal client id, client secret, and tenant id save it in a secure place for later | . Connectivity Test . | Test your connection! | . Step 3: Log in to your lakeFS Cloud Installation . Log in to the installation created during the setup process you’ve followed: . | Navigate to Clusters and click on the link to the newly created installation. | Log in to your lakeFS instance. | . Step 4: Generate lakeFS Credentials . To generate credentials for lakeFS for Databricks: . | In the lakeFS UI, go to the Administration tab. | Click “Create Access Key” | Save the access key and secret key in a secure location | By default, these credentials have admin permissions. If you need more restrictive permissions, create a user with the following steps: . | Go to Administration -&gt; Users | Click “Create API User” and name it “lakeFS for Databricks” | Go to Administration -&gt; Policies and create the policy below: { \"id\": \"lakeFSForDatabricksPolicy\", \"statement\": [ { \"action\": [ \"fs:ListRepositories\", \"fs:ReadRepository\", \"fs:ReadCommit\", \"fs:ListBranches\", \"fs:ListTags\", \"fs:ListObjects\", \"fs:ReadObject\", \"fs:WriteObject\", \"fs:DeleteObject\", \"fs:RevertBranch\", \"fs:ReadBranch\", \"fs:ReadTag\", \"fs:CreateBranch\", \"fs:CreateTag\", \"fs:DeleteBranch\", \"fs:DeleteTag\", \"fs:CreateCommit\" ], \"effect\": \"allow\", \"resource\": \"arn:lakefs:fs:::repository/{repositoryId}\" } ] } . | Go to Administration -&gt; Users | Select “lakeFS for Databricks” | Attach the newly created lakeFSForDatabricksPolicy | . | . ",
    "url": "/getstarted/setup/azure.html#setup-lakefs-cloud",
    
    "relUrl": "/getstarted/setup/azure.html#setup-lakefs-cloud"
  },"14": {
    "doc": "Azure",
    "title": "Configure the lakefs-databricks CLI",
    "content": "Follow the lakefs-databricks configuration guide. ",
    "url": "/getstarted/setup/azure.html#configure-the-lakefs-databricks-cli",
    
    "relUrl": "/getstarted/setup/azure.html#configure-the-lakefs-databricks-cli"
  },"15": {
    "doc": "Azure",
    "title": "Azure",
    "content": " ",
    "url": "/getstarted/setup/azure.html",
    
    "relUrl": "/getstarted/setup/azure.html"
  },"16": {
    "doc": "CLI Reference",
    "title": "lakefs-databricks CLI Reference",
    "content": " ",
    "url": "/cli-reference.html#lakefs-databricks-cli-reference",
    
    "relUrl": "/cli-reference.html#lakefs-databricks-cli-reference"
  },"17": {
    "doc": "CLI Reference",
    "title": "Table of contents",
    "content": ". | Installing lakefs-databricks locally | CLI Configuration | Command Reference | . ",
    "url": "/cli-reference.html#table-of-contents",
    
    "relUrl": "/cli-reference.html#table-of-contents"
  },"18": {
    "doc": "CLI Reference",
    "title": "Installing lakefs-databricks locally",
    "content": "lakefs-databricks is available for Linux, macOS, and Windows. TODO: UPDATE LINKS . lakefs-databricks-aarch64-apple-darwin-0.3.0.tar.gz lakefs-databricks-x86_64-apple-darwin-0.3.0.tar.gz lakefs-databricks-x86_64-pc-windows-msvc-0.3.0.zip lakefs-databricks-x86_64-unknown-linux-gnu-0.3.0.tar.gz lakefs-databricks-x86_64-unknown-linux-musl-0.3.0.tar.gz . ",
    "url": "/cli-reference.html#installing-lakefs-databricks-locally",
    
    "relUrl": "/cli-reference.html#installing-lakefs-databricks-locally"
  },"19": {
    "doc": "CLI Reference",
    "title": "CLI Configuration",
    "content": "lakefs-databricks can be configured using either a YAML file or environment variables. Configuration file . By default, lakefs-databricks will look for a file named .lakefs-databricks.yaml in your home directory, but the configuration file can be specified by passing the -c (or --config) argument to lakefs-databricks (see below). | AWS | Azure | . unity: url: &lt;unity_url&gt; # Your Databricks workspace URL. e.g. https://dbc-&lt;workspace-id&gt;.cloud.databricks.com token: &lt;unity_token&gt; # The Personal access token you created during the Databricks Workspace preparation lakefs: secret_access_key: &lt;lakefs_secret_access_key&gt; # The secret_access_key you created during lakeFS Cloud setup access_key_id: &lt;lakefs_access_key_id&gt; # The access_key_id you created during lakeFS Cloud setup url: &lt;lakefs_endpoint_url&gt;/api/v1 # The Url of your lakeFS Cloud instance e.g. https://example-org.us-east-1.lakefscloud.io databricks_storage: type: s3 s3: access_key_id: &lt;aws_access_key_id&gt; # the access_key you created on the previous step secret_access_key: &lt;aws_secret_access_key&gt; # the secret_access_key you created on the previous step region: &lt;aws_region&gt; . unity: url: &lt;unity_url&gt; # Your Databricks workspace URL. e.g. https://dbc-&lt;workspace-id&gt;.cloud.databricks.com token: &lt;unity_token&gt; # The Personal access token you created during the Databricks Workspace preparation lakefs: secret_access_key: &lt;lakefs_secret_access_key&gt; # The secret_access_key you created during lakeFS Cloud setup access_key_id: &lt;lakefs_access_key_id&gt; # The access_key_id you created during lakeFS Cloud setup url: &lt;lakefs_endpoint_url&gt;/api/v1 # The Url of your lakeFS Cloud instance e.g. https://example-org.us-east-1.lakefscloud.io databricks_storage: type: azure azure: client_id: &lt;client_id&gt; # The client_id from the lakeFS Cloud Terraform output client_secret: &lt;client_secret&gt; # The client_secret from the lakeFS Cloud Terraform output tenant_id: &lt;tenant_id&gt; # The tenant_id from the lakeFS Cloud Terraform output . Environment variables . | AWS | Azure | . LAKEFS_DATABRICKS_UNITY_URL=&lt;unity_url&gt; LAKEFS_DATABRICKS_UNITY_TOKEN=&lt;unity_token&gt; LAKEFS_DATABRICKS_LAKEFS_URL=&lt;lakefs_installation_url&gt;/api/v1 LAKEFS_DATABRICKS_LAKEFS_ACCESS_KEY_ID=&lt;lakefs_access_key_id&gt; LAKEFS_DATABRICKS_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs_secret_access_key&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_TYPE=s3 LAKEFS_DATABRICKS_DATABRICKS_STORAGE_S3_ACCESS_KEY_ID=&lt;aws_access_key_id&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_S3_SECRET_ACCESS_KEY=&lt;aws_secret_access_key&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_S3_REGION=&lt;aws_region&gt; . LAKEFS_DATABRICKS_UNITY_URL=&lt;unity_url&gt; LAKEFS_DATABRICKS_UNITY_TOKEN=&lt;unity_token&gt; LAKEFS_DATABRICKS_LAKEFS_URL=&lt;lakefs_installation_url&gt;/api/v1 LAKEFS_DATABRICKS_LAKEFS_ACCESS_KEY_ID=&lt;lakefs_access_key_id&gt; LAKEFS_DATABRICKS_LAKEFS_SECRET_ACCESS_KEY=&lt;lakefs_secret_access_key&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_TYPE=azure LAKEFS_DATABRICKS_DATABRICKS_STORAGE_AZURE_CLIENT_ID=&lt;client_id&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_AZURE_CLIENT_SECRET=&lt;client_secret&gt; LAKEFS_DATABRICKS_DATABRICKS_STORAGE_AZURE_TENANT_ID=&lt;tenant_id&gt; . ",
    "url": "/cli-reference.html#cli-configuration",
    
    "relUrl": "/cli-reference.html#cli-configuration"
  },"20": {
    "doc": "CLI Reference",
    "title": "Command Reference",
    "content": "Options . -c, --config &lt;FILE&gt; Config file to use -h, --help Print help -V, --version Print version . Usage . lakefs-databricks [OPTIONS] &lt;COMMAND&gt; . Commands . catalog create . Create a versioned catalog . Options . --force Force creation of the catalog even if it already exists. If specified, the lakeFS repository will be re-associated with the provided Unity catalog. This may have undesired effects. --sample-catalog Add sample data to the created catalog . Usage . lakefs-databricks catalog create [OPTIONS] &lt;CATALOG_NAME&gt; &lt;LAKEFS_REPO_NAME&gt; . Examples . Create a catalog named “my-catalog” mapped to the LakeFS repository “my-repo”: . lakefs-databricks catalog create my-catalog my-repo . Force-create the same catalog, with sample data: . lakefs-databricks catalog create --force --sample-catalog my-catalog my-repo . branch create . Create a branch . Usage . lakefs-databricks branch create &lt;LAKEFS_REPO_NAME&gt; &lt;BRANCH_NAME&gt; &lt;SOURCE_BRANCH_NAME&gt; . Examples . Create a branch called “my-branch” from the source branch “main”, on the LakeFS repository “my-repo”: . lakefs-databricks branch create my-repo my-branch main . commit . Record catalog changes . Options . -m, --message &lt;MESSAGE&gt; Commit message . Usage . lakefs-databricks commit [OPTIONS] &lt;LAKEFS_REPO_NAME&gt; &lt;BRANCH_ID&gt; . Examples . Commit the changes on branch “my-branch” and repo “my-repo”, with commit message “Changes”: . lakefs-databricks commit -m \"Changes\" my-repo my-branch . ",
    "url": "/cli-reference.html#command-reference",
    
    "relUrl": "/cli-reference.html#command-reference"
  },"21": {
    "doc": "CLI Reference",
    "title": "CLI Reference",
    "content": " ",
    "url": "/cli-reference.html",
    
    "relUrl": "/cli-reference.html"
  },"22": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/faq.html",
    
    "relUrl": "/faq.html"
  },"23": {
    "doc": "Setup",
    "title": "Setup",
    "content": " ",
    "url": "/getstarted/setup/",
    
    "relUrl": "/getstarted/setup/"
  },"24": {
    "doc": "Setup",
    "title": "AWS",
    "content": "On AWS . ",
    "url": "/getstarted/setup/#aws",
    
    "relUrl": "/getstarted/setup/#aws"
  },"25": {
    "doc": "Setup",
    "title": "Azure",
    "content": "On Azure . ",
    "url": "/getstarted/setup/#azure",
    
    "relUrl": "/getstarted/setup/#azure"
  },"26": {
    "doc": "Get Started",
    "title": "Get Started with lakeFS for Databricks",
    "content": " ",
    "url": "/getstarted/#get-started-with-lakefs-for-databricks",
    
    "relUrl": "/getstarted/#get-started-with-lakefs-for-databricks"
  },"27": {
    "doc": "Get Started",
    "title": "Install",
    "content": "Install lakeFS for Databricks . ",
    "url": "/getstarted/#install",
    
    "relUrl": "/getstarted/#install"
  },"28": {
    "doc": "Get Started",
    "title": "Setup",
    "content": "Setup lakeFS for Databricks . ",
    "url": "/getstarted/#setup",
    
    "relUrl": "/getstarted/#setup"
  },"29": {
    "doc": "Get Started",
    "title": "Try it Out",
    "content": "Start using lakeFS for Databricks . ",
    "url": "/getstarted/#try-it-out",
    
    "relUrl": "/getstarted/#try-it-out"
  },"30": {
    "doc": "Get Started",
    "title": "Get Started",
    "content": " ",
    "url": "/getstarted/",
    
    "relUrl": "/getstarted/"
  },"31": {
    "doc": "lakeFS for Databricks",
    "title": "Welcome to lakeFS for Databricks!",
    "content": ". ",
    "url": "/#welcome-to-lakefs-for-databricks",
    
    "relUrl": "/#welcome-to-lakefs-for-databricks"
  },"32": {
    "doc": "lakeFS for Databricks",
    "title": "What is lakeFS for Databricks?",
    "content": "lakeFS for Databricks is a dataset versioning solution designed for Unity Catalog. It provides the ability to efficiently manage updates and collaborate on production data with ease and confidence. Working at the catalog level, lakeFS for Databricks provides versioning across multiple Delta Lake tables using Git-like semantics. lakeFS for Databricks boosts data teams productivity and reduces time to delivery. It enables them to easily track and manage changes, experiment with data, collaborate effectively, and maintain data integrity and reproducibility. ",
    "url": "/#what-is-lakefs-for-databricks",
    
    "relUrl": "/#what-is-lakefs-for-databricks"
  },"33": {
    "doc": "lakeFS for Databricks",
    "title": "How does lakeFS for Databricks work?",
    "content": "lakeFS for Databricks transforms your Unity Catalog into a versioned catalog by integrating with lakeFS, the robust data versioning engine that powers its capabilities. You continue to work on your Delta Lake tables directly via Unity Catalog using either serverless or standard Databricks compute, while using the lakeFS for Databricks CLI tool for versioning operations such as branching or commiting changes. lakeFS for Databricks leverages Unity schemas to represent lakeFS branches, visible in the Unity Catalog UI. Data reads and writes occur directly through Unity Catalog, with lakeFS for Databricks observing changes rather than interfering in the data path. ",
    "url": "/#how-does-lakefs-for-databricks-work",
    
    "relUrl": "/#how-does-lakefs-for-databricks-work"
  },"34": {
    "doc": "lakeFS for Databricks",
    "title": "Why lakeFS for Databricks?",
    "content": "Test before Run . With lakeFS for Databricks, you can create branches from your production catalogs, allowing you to work on real production data in isolation without duplicating large datasets or relying on partial samples. This lets you modify data pipelines, test outputs, and apply changes confidently. Collaborate . Easily collaborate on shared datasets by creating branches for individual work. Commit your changes and share specific data versions with your team. Reproduce Results . Transform your Unity Catalog into a versioned catalog with lakeFS for Databricks. Each commit captures changes across all Delta Lake tables, allowing you to consistently reproduce results by referencing specific commits. Instant Error Recovery . With lakeFS for Databricks, perform a catalog-level rollback to quickly restore your environment to a previous, healthy state. This multi-table time travel capability ensures quick recovery from errors. ",
    "url": "/#why-lakefs-for-databricks",
    
    "relUrl": "/#why-lakefs-for-databricks"
  },"35": {
    "doc": "lakeFS for Databricks",
    "title": "lakeFS for Databricks",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"36": {
    "doc": "Install",
    "title": "Install",
    "content": "Installing lakeFS for Databricks is a two-step process. First, create a lakeFS Cloud account. Then, download the lakefs-databricks CLI tool. ",
    "url": "/getstarted/install.html",
    
    "relUrl": "/getstarted/install.html"
  },"37": {
    "doc": "Install",
    "title": "Create a lakeFS Cloud Account",
    "content": ". | Register to lakeFS Cloud | Verify your email | . Note To use lakeFS for Databricks, you must verify your lakeFS Cloud account. Account verification enables you to use lakeFS Cloud with your data, which is a required by lakeFS for Databricks. ",
    "url": "/getstarted/install.html#create-a-lakefs-cloud-account",
    
    "relUrl": "/getstarted/install.html#create-a-lakefs-cloud-account"
  },"38": {
    "doc": "Install",
    "title": "Download the lakefs-databricks CLI",
    "content": "Follow this link to download lakefs-databricks, the lakeFS for Databricks CLI tool. ",
    "url": "/getstarted/install.html#download-the-lakefs-databricks-cli",
    
    "relUrl": "/getstarted/install.html#download-the-lakefs-databricks-cli"
  },"39": {
    "doc": "Roadmap",
    "title": "Roadmap",
    "content": " ",
    "url": "/roadmap.html",
    
    "relUrl": "/roadmap.html"
  },"40": {
    "doc": "Try it Out",
    "title": "Try it Out",
    "content": " ",
    "url": "/getstarted/try-it-out.html",
    
    "relUrl": "/getstarted/try-it-out.html"
  },"41": {
    "doc": "Try it Out",
    "title": "With Sample Data",
    "content": " ",
    "url": "/getstarted/try-it-out.html#with-sample-data",
    
    "relUrl": "/getstarted/try-it-out.html#with-sample-data"
  },"42": {
    "doc": "Try it Out",
    "title": "With your Data",
    "content": " ",
    "url": "/getstarted/try-it-out.html#with-your-data",
    
    "relUrl": "/getstarted/try-it-out.html#with-your-data"
  }
}
